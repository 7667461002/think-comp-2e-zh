## 十二、合作进化

在最后一章中，我们提出两个问题，一个来自生物学，一个来自哲学：

+   在生物学中，“利他主义问题”是自然选择与利他主义之间的明显冲突，自然选择表明动物生存在不断竞争的状态中来生存和繁殖，利他主义是许多动物帮助其他动物的倾向，甚至是显然对他们不利。见 <https://en.wikipedia.org/wiki/Altruism_(biology)>。
+   在道德哲学中，人性问题是，人类是否从根本上是善良的，或者邪恶的，或者是由环境塑造的空白状态。见 <https://en.wikipedia.org/wiki/Human_nature>。

我们将用来解决这些问题的工具，（同样）是基于智能体的模拟和博弈论，博弈论是一组抽象模型，旨在描述智能体交互的各种方式。具体来说，我们会考虑囚徒困境。

本章的代码位于`chap12.ipynb`中，该书是本书仓库中的`Jupyter`笔记本。使用此代码的更多信息，请参见第？节。

## 12.1 囚徒困境

囚徒困境是博弈论中的一个话题，但它不是一种有趣的博弈。相反，这种博弈揭示了人类的动机和行为。以下是来自维基百科的它的介绍（<http://en.wikipedia.org/wiki/Prisoner's_dilemma>）：

两名犯罪团伙成员被逮捕并囚禁。每个囚犯都被单独监禁，无法与另一方交流。检察官缺乏足够的证据，来证明这两个人的主要指控。他们希望以较轻的指控被判处两年徒刑。同时，检察官为每个囚犯提供商量的余地。每个囚犯都有机会：（1）通过证明对方犯罪出卖对方，或（2）通过保持沉默与另一方合作。出价是：

+   如果 A 和 B 各自背叛对方，每个人都服刑 2 年。
+   如果 A 背叛 B 但 B 保持沉默，A 将被释放，B 将被监禁 3 年（反之亦然）。
+   如果 A 和 B 都保持沉默，他们两人只会服刑 1 年（较轻的质控）。

很显然，这种情况是假想的，但它用于代表各种不同的互动，其中智能体必须选择是相互“合作”还是“背叛”，以及每个智能体的奖励（或惩罚）取决于他人的选择。

有了这套奖惩，我们很有可能说智能体应该合作，也就是说，双方都应该保持沉默。 但两个智能体不知道对方会做什么，所以每个人都必须考虑两种可能的结果。 首先，从 A 的角度来看它：

+   如果 B 保持沉默，A 最好是背叛；他会无罪而不是服刑 1 年。
+   如果 B 背叛，A 最好也是背叛；他只会服刑 2 年而不是 3 年。

不管 B 做什么，A 最好都是背叛。 而且因为博弈是对称的，所以从 B 的角度来看这个分析是一样的：不管 A 做什么，B 最好也是背叛。

在这个博弈的最简单版本中，我们假设 A 和 B 没有考虑其他因素。 他们不能互相沟通，所以他们不能协商，作出承诺或相互威胁。 他们只考虑直接目标，最小化他们的判决；他们不考虑任何其他因素。

在这些假设下，两个智能体的理性选择都是背叛。 这可能是一件好事，至少在刑事司法方面是这样。 但对囚犯来说，这令人沮丧，因为显然，他们无法获得他们双方都想要的结果。 而且这种模式适用于现实生活中的其他场合，其中合作有更大的好处以及对于玩家来说都会更好。

研究这些场景以及摆脱困境的方法，博弈论研究者关注的焦点，但这不是本章的重点。 我们正朝着不同的方向前进。

## 12.2 善良的问题

自 20 世纪 50 年代，囚徒困境被首次讨论以来，它一直是社会心理学研究的热门话题。根据前一节的分析，我们可以说一个理想的智能体应该做什么; 很难预测真正的人究竟做了些什么。 幸运的是，实验已经完成了 [1]。

如果我们假设人们足够聪明地进行分析（或者在解释时理解它），并且他们通常为了自己的利益而行事，那么我们预计他们几乎总是背叛。 但他们没有。 在大多数实验中，受试者的合作远远超过理性的智能体模型的预测 [2]。

这个结果最明显的解释是，人们不是理性的智能体，这对任何人都不应该感到惊讶。 但为什么不是呢？ 是因为他们不够聪明，无法理解这种情况，还是因为他们故意违背自己的利益行事？

根据实验结果，似乎至少有一部分解释是纯粹的利他主义：许多人愿意为了让别人受益而承担成本。现在，在你提出《Journal of Obvious Results》上发表的结论之前，让我们继续问为什么：

+   为什么人们会帮助别人，即使自己会付出代价？至少部分原因是他们想这样；这让他们对自己和世界感觉良好。
+   为什么善良让人感觉良好？诱人的说法是，有人跟他们提出这是正确的，或者更普遍来说，他们被社会训练为想要做好事。但毫无疑问，至少有一大部分利他主义是天生的；在不同程度上，利他主义的倾向是正常大脑发育的结果。
+   那么，为什么呢？大脑发育的内在部分，以及随后的个体特征，是基因的结果。当然，基因与利他主义的关系是复杂的，可能有许多基因与环境因素相互作用，导致人们在不同情况下或多或少是无私的。尽管如此，几乎可以肯定的是基因导致人们变得无私。
+   最后，为什么呢？如果在自然选择下，动物为了生存和繁殖而彼此不断竞争，似乎显然利他主义会适得其反。在一个种群中，有些人帮助别人，甚至是为别人伤害自己，其他人纯粹是自私的，似乎自私者会受益，利他者会受到影响，并且利他主义的基因将被驱逐而灭绝。

这个明显的矛盾是“利他主义问题”：为什么利他主义的基因没有消失？

在生物学家中，有很多可能的解释，包括互惠利他主义，性选择，亲属选择和群体选择。而在非科学家中，还有更多的解释。我把它交给你去探索别的假说；现在我想专注于一种解释，可以说是最简单的一种解释：也许利他主义是适应性的。换句话说，利他主义的基因可能使人们更容易生存和繁殖。

事实证明，引发利他主义问题的囚徒困境，也可能有助于解决问题。

## 12.3 囚徒困境锦标赛

在 20 世纪 70 年代后期，密歇根大学的政治学家罗伯特阿克塞尔罗德（Robert Axelrod）组织了一场比赛来比较囚徒困境（PD）的策略。

他邀请参与者以计算机程序的形式提交策略，然后相互对抗并保持得分。具体来说，他们玩的是 PD 的迭代版本，其中智能体针对同一对手进行多轮比赛，因此他们的决定可以基于历史。

在 Axelrod 的比赛中，一个简单的策略出人意料地好，称为“针锋相对”，即 TFT，TFT 在第一轮迭代比赛中总是合作；之后，它会复制上一轮对手所做的任何事情。对手继续合作，TFT 保持合作，如果对手任何时候都背叛，下一轮 TFT 背叛，但如果对手变回合作，TFT 也会合作。

这些锦标赛的更多信息，以及 TFT 为何如此出色的解释，请参阅以下视频：<https://www.youtube.com/watch?v=BOvAbjfJ0x0>。

看看这些锦标赛中表现出色的策略，Alexrod 发现了他们倾向于分享的特点：

+   善良：表现好的策略在第一轮比赛中合作，并且通常会在随后的几轮中合作。
+   报复：始终合作的策略，并不如如果对手背叛就报复的策略好。
+   宽恕：但是过于斗气的策略往往会惩罚自己以及对手。
+   不嫉妒：一些最成功的策略很少超过对手；他们成功了，因为他们对各种各样的对手都做得足够好。

TFT 具有所有这些属性。

Axelrod 的比赛为利他主义问题提供了部分可能的答案：也许利他主义的基因是普遍存在的，因为它们是适应性的。 许多社会互动可以建模为囚徒困境的变种，就这种程度而言，如果将一个大脑设定为善良，平衡报复和宽恕，就会在各种各样的情况下表现良好。

但是 Axelrod 比赛中的策略是由人们设计的；他们并不进化。 我们需要考虑，善良、报复和宽恕的基因是否可以通过突变出现，成功侵入其他策略的种群，并抵制后续突变的侵入。

## 12.4 合作进化的模拟

合作进化是第一本书的标题，Axelrod 展示了来自囚徒困境比赛的结果，并讨论了利他主义问题的影响。 从那以后，他和其他研究人员已经探索了 PD 比赛的进化动态性，也就是说，PD 选手的总体中，策略的分布随时间如何变化。 在本章的其余部分中，我运行这些实验的一个版本并展示结果。

首先，我们需要一种将 PD 策略编码为基因型的方法。 在这个实验中，我考虑了一些策略，其中智能体每一轮的选择仅取决于前两轮中对手的选择。 我用字典来表示策略，它将对手的前两个选择映射为智能体的下一个选择。

以下是这些智能体的类定义：

```py
class Agent:

    keys = [(None, None),
            (None, 'C'),
            (None, 'D'),
            ('C', 'C'),
            ('C', 'D'),
            ('D', 'C'),
            ('D', 'D')]

    def __init__(self, values, fitness=np.nan):
        self.values = values
        self.responses = dict(zip(self.keys, values))
        self.fitness = fitness
```

`keys`是每个智能体的词典中的键序列，其中元组`('C', 'C')`表示对手在前两轮合作；`(None, 'C')`意味着只有一轮比赛并且对手合作；`(None, None)`表示还没有回合。

在`__init__`方法中，`values `是对应于键的一系列选项，`'C'`或`'D'`。 所以如果值的第一个元素是`'C'`，那就意味着这个智能体将在第一轮合作。 如果值的最后一个元素是`'D'`，那么如果对手在前两轮中背叛，该智能体将会背叛。

在这个实现中，总是背叛的智能体的基因型是`'DDDDDDD'`; 总是合作的智能体的基因型是`'CCCCCCC'`，而 TFT 的基因型是`'CCDCDCD'`。

`Agent`类提供`copy`，它使其它智能体具有相同的基因型，但具有一定的变异概率：

```py

prob_mutate = 0.05

def copy(self):
    if np.random.random() > self.prob_mutate:
        values = self.values
    else:
        values = self.mutate()
    return Agent(values, self.fitness)
```

突变的原理是，在基因型中选择一个随机值并从`'C'`翻转到`'D'`，或者相反：

```py

def mutate(self):
    values = list(self.values)
    index = np.random.choice(len(values))
    values[index] = 'C' if values[index] == 'D' else 'D'
    return values
```

既然我们有了智能体，我们还需要锦标赛。
